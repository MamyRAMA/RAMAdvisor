#!/usr/bin/env python3
"""
Script de gÃ©nÃ©ration d'embeddings pour le cours CFA Advanced Private Wealth Management
AdaptÃ© pour RAMAdvisor - GÃ©nÃ¨re des embeddings statiques pour Netlify Functions

USAGE:
    python generate_cfa_embeddings.py

SORTIE:
    - cfa_knowledge_embeddings.json : Embeddings + mÃ©tadonnÃ©es du cours CFA
    - cfa_embedding_config.json : Configuration du modÃ¨le
    - cfa_search_index.json : Index de recherche rapide

OBJECTIF:
    IntÃ©grer la connaissance professionnelle de gestion privÃ©e du CFA
    dans le systÃ¨me de recommandations d'investissement
"""

import os
import json
import re
from pathlib import Path
from typing import List, Dict, Any, Optional
import logging
from dataclasses import dataclass, asdict

import numpy as np
from sentence_transformers import SentenceTransformer
import PyPDF2

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class CFAKnowledgeChunk:
    """ReprÃ©sente un fragment de connaissance CFA avec mÃ©tadonnÃ©es."""
    text: str
    source_file: str = "course.pdf"
    page_number: Optional[int] = None
    chunk_index: int = 0
    topic_category: Optional[str] = None  # Ex: "Asset Allocation", "Risk Management"
    relevance_keywords: List[str] = None
    embedding: Optional[List[float]] = None
    
    def __post_init__(self):
        if self.relevance_keywords is None:
            self.relevance_keywords = []

class CFAEmbeddingGenerator:
    """GÃ©nÃ©rateur d'embeddings spÃ©cialisÃ© pour la connaissance CFA."""
    
    def __init__(self, 
                 pdf_path: str = "../docs/knowledge/course.pdf",
                 output_dir: str = "../netlify/functions/cfa_data",
                 model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        """
        Initialise le gÃ©nÃ©rateur d'embeddings CFA.
        
        Args:
            pdf_path: Chemin vers le PDF du cours CFA
            output_dir: RÃ©pertoire de sortie pour Netlify Functions
            model_name: ModÃ¨le Sentence Transformers optimisÃ©
        """
        self.pdf_path = Path(pdf_path)
        self.output_dir = Path(output_dir)
        self.model_name = model_name
        
        # CrÃ©er le rÃ©pertoire de sortie
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Charger le modÃ¨le d'embeddings
        logger.info(f"Chargement du modÃ¨le d'embeddings: {model_name}")
        self.embedding_model = SentenceTransformer(model_name)
        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
        
        self.chunks: List[CFAKnowledgeChunk] = []
        
        # Mots-clÃ©s pour catÃ©goriser les sujets CFA
        self.topic_keywords = {
            "Asset Allocation": ["asset allocation", "portfolio", "diversification", "strategic allocation", "tactical allocation"],
            "Risk Management": ["risk", "volatility", "downside", "var", "risk tolerance", "risk capacity"],
            "Investment Strategy": ["strategy", "investment", "approach", "methodology", "framework"],
            "Client Management": ["client", "advisor", "relationship", "communication", "objectives"],
            "Performance": ["performance", "return", "benchmark", "measurement", "evaluation"],
            "Tax Planning": ["tax", "taxation", "tax-efficient", "after-tax", "tax planning"],
            "Estate Planning": ["estate", "inheritance", "succession", "wealth transfer", "legacy"],
            "Alternative Investments": ["alternative", "hedge fund", "private equity", "real estate", "commodities"]
        }
    
    def extract_text_from_pdf(self) -> List[Dict[str, Any]]:
        """Extrait le texte du PDF CFA avec optimisations pour le contenu acadÃ©mique."""
        text_chunks = []
        try:
            with open(self.pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                total_pages = len(pdf_reader.pages)
                logger.info(f"Traitement du cours CFA: {total_pages} pages")
                
                for page_num, page in enumerate(pdf_reader.pages, 1):
                    text = page.extract_text()
                    if text.strip():
                        # Nettoyage spÃ©cialisÃ© pour contenu acadÃ©mique
                        text = self.clean_academic_text(text)
                        if len(text) > 100:  # Filtrer les pages avec peu de contenu
                            text_chunks.append({
                                'text': text,
                                'page_number': page_num
                            })
                            logger.info(f"Page {page_num}/{total_pages}: {len(text)} caractÃ¨res extraits")
                        
        except Exception as e:
            logger.error(f"Erreur lors de l'extraction PDF {self.pdf_path}: {e}")
        
        logger.info(f"Extraction terminÃ©e: {len(text_chunks)} pages valides")
        return text_chunks
    
    def clean_academic_text(self, text: str) -> str:
        """Nettoie le texte acadÃ©mique pour optimiser la qualitÃ© des embeddings."""
        # Supprimer les caractÃ¨res de mise en page
        text = re.sub(r'\s+', ' ', text)
        
        # Supprimer les numÃ©ros de page et headers/footers typiques
        text = re.sub(r'Page \d+', '', text)
        text = re.sub(r'Chapter \d+', '', text)
        text = re.sub(r'CFA Institute', '', text)
        
        # Nettoyer les rÃ©fÃ©rences de formules mathÃ©matiques isolÃ©es
        text = re.sub(r'\b[A-Z]\s*=\s*[A-Z]\s*[+\-\*/]\s*[A-Z]\b', '', text)
        
        # Conserver la structure des phrases
        text = re.sub(r'([.!?])\s*([A-Z])', r'\1 \2', text)
        
        return text.strip()
    
    def categorize_chunk(self, text: str) -> Optional[str]:
        """CatÃ©gorise un chunk selon les sujets CFA principaux."""
        text_lower = text.lower()
        
        # Compter les occurrences de mots-clÃ©s par catÃ©gorie
        category_scores = {}
        for category, keywords in self.topic_keywords.items():
            score = sum(text_lower.count(keyword) for keyword in keywords)
            if score > 0:
                category_scores[category] = score
        
        # Retourner la catÃ©gorie avec le score le plus Ã©levÃ©
        if category_scores:
            return max(category_scores, key=category_scores.get)
        return None
    
    def extract_keywords(self, text: str) -> List[str]:
        """Extrait les mots-clÃ©s pertinents pour la recherche."""
        # Mots-clÃ©s financiers importants
        financial_terms = {
            "portfolio", "diversification", "allocation", "risk", "return", "volatility",
            "equity", "bond", "asset", "investment", "strategy", "client", "advisor",
            "wealth", "management", "planning", "tax", "estate", "performance",
            "benchmark", "correlation", "sharpe", "alpha", "beta", "hedge"
        }
        
        text_lower = text.lower()
        found_keywords = []
        
        for term in financial_terms:
            if term in text_lower:
                found_keywords.append(term)
        
        return found_keywords[:10]  # Limiter Ã  10 mots-clÃ©s par chunk
    
    def chunk_text_smart(self, text: str, chunk_size: int = 500, overlap: int = 100) -> List[str]:
        """
        Divise intelligemment le texte acadÃ©mique en chunks avec contexte.
        
        Args:
            text: Texte Ã  diviser
            chunk_size: Taille cible d'un chunk
            overlap: Chevauchement entre chunks
        """
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        sentences = re.split(r'[.!?]+', text)
        
        current_chunk = ""
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            # Si ajouter cette phrase dÃ©passe la taille, finaliser le chunk
            if len(current_chunk) + len(sentence) > chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                # Garder un overlap en conservant les derniÃ¨res phrases
                overlap_text = '. '.join(current_chunk.split('. ')[-2:])
                current_chunk = overlap_text + ". " + sentence
            else:
                current_chunk += ". " + sentence if current_chunk else sentence
        
        # Ajouter le dernier chunk
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        # Filtrer les chunks trop courts
        return [chunk for chunk in chunks if len(chunk) > 100]
    
    def process_cfa_document(self) -> int:
        """Traite le document CFA et crÃ©e les chunks enrichis."""
        logger.info(f"Traitement du cours CFA: {self.pdf_path}")
        
        if not self.pdf_path.exists():
            logger.error(f"Fichier PDF CFA non trouvÃ©: {self.pdf_path}")
            return 0
        
        # Extraire le texte du PDF
        text_data = self.extract_text_from_pdf()
        
        if not text_data:
            logger.error("Aucun texte extrait du PDF CFA")
            return 0
        
        # Traiter chaque page
        chunk_counter = 0
        for page_data in text_data:
            text = page_data['text']
            page_num = page_data['page_number']
            
            # Diviser en chunks intelligents
            chunks = self.chunk_text_smart(text, chunk_size=450, overlap=80)
            
            for i, chunk_text in enumerate(chunks):
                # CrÃ©er le chunk enrichi
                chunk = CFAKnowledgeChunk(
                    text=chunk_text,
                    page_number=page_num,
                    chunk_index=chunk_counter,
                    topic_category=self.categorize_chunk(chunk_text),
                    relevance_keywords=self.extract_keywords(chunk_text)
                )
                self.chunks.append(chunk)
                chunk_counter += 1
        
        logger.info(f"Document CFA traitÃ©: {len(self.chunks)} chunks crÃ©Ã©s")
        return len(self.chunks)
    
    def generate_embeddings(self):
        """GÃ©nÃ¨re les embeddings pour tous les chunks CFA."""
        if not self.chunks:
            logger.warning("Aucun chunk CFA Ã  traiter")
            return
        
        logger.info("GÃ©nÃ©ration des embeddings CFA...")
        texts = [chunk.text for chunk in self.chunks]
        
        # GÃ©nÃ©rer les embeddings par batch
        batch_size = 8  # Plus petit pour Ã©viter les timeouts
        embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(texts) - 1) // batch_size + 1
            logger.info(f"Traitement batch {batch_num}/{total_batches}")
            
            batch_embeddings = self.embedding_model.encode(
                batch, 
                convert_to_tensor=False,
                normalize_embeddings=True
            )
            embeddings.extend(batch_embeddings)
        
        # Assigner les embeddings aux chunks
        for chunk, embedding in zip(self.chunks, embeddings):
            chunk.embedding = embedding.tolist()
        
        logger.info(f"Embeddings CFA gÃ©nÃ©rÃ©s pour {len(self.chunks)} chunks")
    
    def save_cfa_data(self):
        """Sauvegarde les donnÃ©es CFA pour Netlify Functions."""
        logger.info("Sauvegarde des donnÃ©es CFA...")
        
        # 1. Fichier principal des embeddings
        embeddings_data = []
        for chunk in self.chunks:
            chunk_dict = asdict(chunk)
            embeddings_data.append(chunk_dict)
        
        embeddings_file = self.output_dir / "cfa_knowledge_embeddings.json"
        with open(embeddings_file, 'w', encoding='utf-8') as f:
            json.dump(embeddings_data, f, ensure_ascii=False, indent=2)
        logger.info(f"Embeddings sauvegardÃ©s: {embeddings_file} ({len(embeddings_data)} chunks)")
        
        # 2. Configuration du modÃ¨le
        config_data = {
            "model_name": self.model_name,
            "embedding_dim": self.embedding_dim,
            "total_chunks": len(self.chunks),
            "source_file": "course.pdf",
            "generated_at": str(Path(__file__).stat().st_mtime),
            "categories": list(self.topic_keywords.keys())
        }
        
        config_file = self.output_dir / "cfa_embedding_config.json"
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, indent=2)
        logger.info(f"Configuration sauvegardÃ©e: {config_file}")
        
        # 3. Index de recherche rapide (mots-clÃ©s)
        search_index = {}
        for chunk in self.chunks:
            for keyword in chunk.relevance_keywords:
                if keyword not in search_index:
                    search_index[keyword] = []
                search_index[keyword].append(chunk.chunk_index)
        
        index_file = self.output_dir / "cfa_search_index.json"
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(search_index, f, indent=2)
        logger.info(f"Index de recherche sauvegardÃ©: {index_file}")
        
        # 4. Statistiques
        stats = {
            "total_chunks": len(self.chunks),
            "categories_distribution": {},
            "average_chunk_length": sum(len(chunk.text) for chunk in self.chunks) / len(self.chunks),
            "total_keywords": len(search_index)
        }
        
        for chunk in self.chunks:
            cat = chunk.topic_category or "Uncategorized"
            stats["categories_distribution"][cat] = stats["categories_distribution"].get(cat, 0) + 1
        
        stats_file = self.output_dir / "cfa_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2)
        logger.info(f"Statistiques sauvegardÃ©es: {stats_file}")
        
        return {
            "embeddings_file": str(embeddings_file),
            "config_file": str(config_file), 
            "index_file": str(index_file),
            "stats_file": str(stats_file)
        }
    
    def run_complete_pipeline(self) -> Dict[str, Any]:
        """ExÃ©cute le pipeline complet de gÃ©nÃ©ration des embeddings CFA."""
        logger.info("ğŸš€ DÃ©marrage du pipeline CFA RAG")
        
        # Ã‰tape 1: Traitement du document
        chunks_created = self.process_cfa_document()
        if chunks_created == 0:
            raise RuntimeError("Aucun chunk crÃ©Ã© - arrÃªt du pipeline")
        
        # Ã‰tape 2: GÃ©nÃ©ration des embeddings
        self.generate_embeddings()
        
        # Ã‰tape 3: Sauvegarde
        file_paths = self.save_cfa_data()
        
        logger.info("âœ… Pipeline CFA RAG terminÃ© avec succÃ¨s")
        return {
            "chunks_processed": chunks_created,
            "embedding_dimension": self.embedding_dim,
            "files_created": file_paths
        }

def main():
    """Point d'entrÃ©e principal."""
    try:
        generator = CFAEmbeddingGenerator()
        results = generator.run_complete_pipeline()
        
        print("\n" + "="*60)
        print("ğŸ“ GÃ‰NÃ‰RATION EMBEDDINGS CFA - RÃ‰SULTATS")
        print("="*60)
        print(f"âœ… Chunks traitÃ©s: {results['chunks_processed']}")
        print(f"âœ… Dimension embeddings: {results['embedding_dimension']}")
        print(f"âœ… Fichiers crÃ©Ã©s:")
        for purpose, filepath in results['files_created'].items():
            print(f"   - {purpose}: {filepath}")
        print("\nğŸ”— PrÃªt pour intÃ©gration dans Netlify Functions!")
        
    except Exception as e:
        logger.error(f"âŒ Erreur dans le pipeline: {e}")
        raise

if __name__ == "__main__":
    main()
